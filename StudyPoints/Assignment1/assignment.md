# Analyzing Time Complexity

## Task 2 - Bubble Sort Algorithm

Bubble sort is a very simple sorting algorithm that sorts an array by iterating over it and pushing the largest element to the end. This step is then repeated until the array has been sorted. Big O is a useful mathematical notation that can be used to describe the upper bound on the growth of a function's running time and, or its space complexity. This is done by identifying the number of operations the algorithm performs on its input and then simplifying the expression by removing the constants and the lower-order terms. Big O is especially useful when comparing other algorithms with each other since it gives us a scientific metric to compare with, this lets us choose the more efficient algorithm for a given problem. Furthermore, the Big O notation also allows us to identify which parts of the algorithm that contribute most to its time complexity. In the case of bubble sort, the nested loop is the most time-consuming part of the algorithm. By knowing this we can try to optimize the algorithm by reducing the number of comparisons and swaps within the loop which can lead to significant improvements in performance in most cases although the algorithm will still continue to have a complexity of O(n^2) until we remove the nested loop.
In the case of my bubble sort algorithm I start by identify the input size of the array n. I then go through the algorithm and find the basic operations that are performed repeatedly. In the case of the bubble sort algorithm it would be the comparison on line 5 and the swap operation on line 6. I then count the number of times these operations are performed in the following way:

- I start by looking at how many times the basic operations occur in the algorithm. Both the comparison and the swap occurs one time. 

- I now look at the inner array which will execute n-i-1 times, where i is the current iteration of the outer loop e.i. for each iteration of the outer loop the inner loop will iterate over one less item. But on the first run the inner loop will run n-0-1 times e.i. n-1

- Lastly I look at the outer loop which runs a total of n times, since the array will be sorted when we have tried to push all the elements to the back.

We multiply the two terms from the loops together since the inner loop will repeat as many times as the outer loop repeats. This gives us n*(n-1)/2

These three numbers can then be combined into a single equation and expressed in the following way n*(n-1)/2*1*1=(n^2-n)/2*1*1. I now simplify the expression by removing the constants and the lower-order terms which result in n^2 or written with big O notation O(n^2).
Based on our analysis we now know that the time complexity of the bubble sort algorithm is O(n^2) which means that as the size n of the input array grows, the time the algorithm takes increases quadratically.

## Task 3 - Stack Data Structure

A Stack is a data structure that serves as a collection of elements and follows the LIFO (last in, first out) principle. Items can be inserted ("pushed"), deleted ("popped") and sometimes looked at without deleting ("peeked") but only the most recently inserted element can be operated on. As in the case of analyzing the complexity of algorithms, big O can also be used to analyze the complexity of data structures. However, since a data structure simply is a way of managing and storing data we can't just count the basic operations like we did with our bubble sort example. Instead, to get an idea of a data structures complexity we measure the performance of common operations that are performed on the data structure and then compare this result to get an understanding of the data structures strengths and weaknesses. The most basic operations on a data structure are insertion, deletion, access, and search, some may not be necessary for all data structures like the search operation in the case of a stack where we only interact with the first element.
It's possible to implement the stack data structure in two different ways, the first is to use an array and the second is with the use of a linked list. I have opted for the second approach since it simplifies things like resizing the array when it becomes full and keeping track of the current position in the array. 

- Insertion – The only way to insert an item into a stack is by using its push operation that inserts the new element at the top of the stack. This can be done in constant time, O(1), in the case of the linked list implementation since we just have to update the head pointer to the new node. However, when it comes to a stack with an array implementation, the time varies depending on if we dynamically resize the array when it becomes full or if we use a predefined max size for the stack. The time complexity for the push operation would be O(n) or O(1) respectively.

- Deletion – The pop operation both removes the newest item and returns it. Like in the case of the insertion operation this can be done in constant time, O(1), with a linked list implementation and depends when it comes to the array implementation.

- Access – pop already returns the first item in constant time while also deleting the item, so this would be the same for a peek operation that only has to return the item and not delete it. However, if we wanted to access a specific element in the stack, that would take time complexity would again vary depending on the underlying implementation. A linked list doesn’t have an index and would therefore need to iterate over the whole stack to find the last item. This gives it a time complexity of O(n). But in the case of an array implementation, we could just access the item based on its index in the underlying array and thereby have a constant time complexity of O(1).

- Search – The stack usually only allows for interaction with the newest inserted item, however, it can still be useful to know whether an item is in the stack or not so items aren't added twice. The search operation would have a time complexity of O(n) since it would need to iterate over each item in the stack before it finds the item, and in the worst case the item that we are looking for is at the bottom.

## Task 5 - Recursive Function

A function that makes calls to itself is said to be recursive, these types of functions are particularly useful for solving problems that can be broken down into smaller instances of itself, solved, and then put back together. Just as in the previous examples, knowing the big O of a recursive function can help us make an informed decision about whether an algorithm is appropriate for a given problem and it can help us identify opportunities for optimization. Since recursive functions call themself they can easily have a large growth rate as the input size increases which makes knowing the worst case situation, e.i. big O, for the algorithm quite important. 
To calculate big O for a naive fibonacci sequence function I first want to define some useful constants. The first one is the size of the input, n, which is also the number of the Fibonacci sequence I want to compute. I then count the max number of recursion calls in a single instance of the function, this is 2 in the case of the fibonacci function. Finally I try to figure out how much n decreases on each recursion call. There are two values to choose from since n both are decreased with -1 and -2 respectively, Im looking for the worst case scenario which is -1. 
I can now use these three constants to calculate big O for the recursive function. Since we only decrease our input value with 1 each time we have to make a minimum of n recursion calls before the function returns and each time we make a recursion call we make two new ones. This can be express with the equation 2^n, which means that the big O of this naive fibonacci calculator is O(2^n).
